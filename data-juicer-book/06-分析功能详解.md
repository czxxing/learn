# 第6章：分析功能详解

[← 上一章](05-模型调用机制.md) | [返回目录](00-目录与索引.md) | [下一章 →](07-总结与展望.md)

---

## 6.1 分析系统架构设计

Data-Juicer的分析功能提供了对数据集进行深度统计和可视化的能力，帮助用户理解数据特征、发现数据质量问题，并为数据处理策略提供决策支持。

### 整体架构概览

分析系统的核心架构包括以下关键组件：

- **Analyzer基类**：定义分析器的统一接口和行为
- **分析器注册机制**：支持动态注册和发现分析器
- **统计信息收集器**：负责收集和聚合分析结果
- **可视化引擎**：将分析结果转换为可视化图表
- **报告生成器**：生成详细的分析报告

### 核心类设计

```python
class Analyzer(metaclass=ABCMeta):
    """分析器抽象基类"""
    
    @abstractmethod
    def analyze(self, dataset, **kwargs):
        """分析数据集"""
        pass
    
    @abstractmethod
    def get_stats(self):
        """获取分析统计信息"""
        pass
    
    @abstractmethod
    def visualize(self, **kwargs):
        """可视化分析结果"""
        pass
```

## 6.2 分析器类型与实现

### 6.2.1 整体分析（OverallAnalysis）

整体分析提供数据集的宏观统计信息，包括数据规模、质量指标等。

```python
class OverallAnalysis(Analyzer):
    """整体分析器"""
    
    def __init__(self):
        self.stats = {
            "dataset_size": 0,
            "total_tokens": 0,
            "avg_text_length": 0,
            "quality_scores": {},
            "data_distribution": {}
        }
    
    def analyze(self, dataset, **kwargs):
        """执行整体分析"""
        
        # 重置统计信息
        self.stats = {
            "dataset_size": len(dataset),
            "total_tokens": 0,
            "avg_text_length": 0,
            "quality_scores": {},
            "data_distribution": {}
        }
        
        # 收集文本统计信息
        text_lengths = []
        token_counts = []
        
        for sample in dataset:
            text = sample.get("text", "")
            
            # 文本长度统计
            text_lengths.append(len(text))
            
            # 分词统计（如果支持）
            if hasattr(self, 'tokenizer'):
                tokens = self.tokenizer.tokenize(text)
                token_counts.append(len(tokens))
        
        # 计算统计指标
        self.stats["avg_text_length"] = sum(text_lengths) / len(text_lengths)
        self.stats["total_tokens"] = sum(token_counts)
        
        # 数据分布分析
        self._analyze_distribution(dataset)
        
        return self.stats
    
    def _analyze_distribution(self, dataset):
        """分析数据分布"""
        # 文本长度分布
        length_distribution = {}
        for sample in dataset:
            text = sample.get("text", "")
            length_bucket = len(text) // 100 * 100  # 按100字符分桶
            length_distribution[length_bucket] = length_distribution.get(length_bucket, 0) + 1
        
        self.stats["data_distribution"]["text_length"] = length_distribution
```

### 6.2.2 列级分析（ColumnWiseAnalysis）

列级分析针对数据集的特定列进行深入分析，支持文本、数值、分类等多种数据类型。

```python
class ColumnWiseAnalysis(Analyzer):
    """列级分析器"""
    
    def __init__(self, columns=None):
        self.columns = columns or []
        self.stats = {}
    
    def analyze(self, dataset, **kwargs):
        """执行列级分析"""
        
        for column in self.columns:
            if column not in dataset.column_names:
                logger.warning(f"Column {column} not found in dataset")
                continue
            
            # 分析特定列
            column_stats = self._analyze_column(dataset, column)
            self.stats[column] = column_stats
        
        return self.stats
    
    def _analyze_column(self, dataset, column):
        """分析单个列"""
        column_data = dataset[column]
        
        # 检测数据类型
        data_type = self._detect_data_type(column_data)
        
        # 根据数据类型执行相应分析
        if data_type == "text":
            return self._analyze_text_column(column_data)
        elif data_type == "numeric":
            return self._analyze_numeric_column(column_data)
        elif data_type == "categorical":
            return self._analyze_categorical_column(column_data)
        else:
            return {"type": "unknown", "count": len(column_data)}
    
    def _detect_data_type(self, data):
        """检测数据类型"""
        sample = data[0] if data else None
        
        if isinstance(sample, str):
            # 检查是否为数值字符串
            try:
                float(sample)
                return "numeric"
            except ValueError:
                # 检查是否为分类数据
                unique_values = len(set(data))
                if unique_values < len(data) * 0.1:  # 唯一值少于10%
                    return "categorical"
                else:
                    return "text"
        elif isinstance(sample, (int, float)):
            return "numeric"
        else:
            return "unknown"
    
    def _analyze_text_column(self, data):
        """分析文本列"""
        stats = {
            "type": "text",
            "count": len(data),
            "non_empty_count": sum(1 for x in data if x),
            "avg_length": 0,
            "unique_count": len(set(data)),
            "common_words": {}
        }
        
        # 计算平均长度
        lengths = [len(str(x)) for x in data if x]
        if lengths:
            stats["avg_length"] = sum(lengths) / len(lengths)
        
        # 分析常用词汇
        all_text = " ".join(str(x) for x in data if x)
        words = all_text.split()
        word_counts = Counter(words)
        stats["common_words"] = dict(word_counts.most_common(10))
        
        return stats
```

### 6.2.3 相关性分析（CorrelationAnalysis）

相关性分析探索数据集中不同特征之间的关系，帮助发现潜在的模式和关联。

```python
class CorrelationAnalysis(Analyzer):
    """相关性分析器"""
    
    def __init__(self, features=None):
        self.features = features or []
        self.stats = {}
    
    def analyze(self, dataset, **kwargs):
        """执行相关性分析"""
        
        # 准备特征数据
        feature_data = self._prepare_feature_data(dataset)
        
        # 计算相关性矩阵
        correlation_matrix = self._compute_correlation_matrix(feature_data)
        
        # 识别强相关性
        strong_correlations = self._identify_strong_correlations(correlation_matrix)
        
        self.stats = {
            "correlation_matrix": correlation_matrix,
            "strong_correlations": strong_correlations,
            "feature_importance": self._compute_feature_importance(feature_data)
        }
        
        return self.stats
    
    def _prepare_feature_data(self, dataset):
        """准备特征数据"""
        feature_data = {}
        
        for feature in self.features:
            if feature in dataset.column_names:
                # 数值化特征值
                values = self._numericalize_feature(dataset[feature])
                feature_data[feature] = values
        
        return feature_data
    
    def _numericalize_feature(self, data):
        """将特征值转换为数值"""
        numerical_data = []
        
        for value in data:
            if isinstance(value, (int, float)):
                numerical_data.append(value)
            elif isinstance(value, str):
                # 字符串特征：使用长度或编码
                numerical_data.append(len(value))
            else:
                numerical_data.append(0)  # 默认值
        
        return numerical_data
    
    def _compute_correlation_matrix(self, feature_data):
        """计算相关性矩阵"""
        import numpy as np
        
        features = list(feature_data.keys())
        n_features = len(features)
        
        # 创建数据矩阵
        data_matrix = np.array([feature_data[f] for f in features]).T
        
        # 计算皮尔逊相关系数
        correlation_matrix = np.corrcoef(data_matrix, rowvar=False)
        
        # 转换为字典格式
        corr_dict = {}
        for i, f1 in enumerate(features):
            corr_dict[f1] = {}
            for j, f2 in enumerate(features):
                corr_dict[f1][f2] = correlation_matrix[i, j]
        
        return corr_dict
```

## 6.3 分析流程执行机制

### 6.3.1 分析器注册与发现

Data-Juicer使用统一的注册机制管理分析器：

```python
class AnalyzerRegistry:
    """分析器注册表"""
    
    _analyzers = {}
    
    @classmethod
    def register(cls, name):
        """注册分析器装饰器"""
        def decorator(analyzer_class):
            cls._analyzers[name] = analyzer_class
            return analyzer_class
        return decorator
    
    @classmethod
    def get_analyzer(cls, name, **kwargs):
        """获取分析器实例"""
        if name not in cls._analyzers:
            raise ValueError(f"Analyzer {name} not found")
        
        analyzer_class = cls._analyzers[name]
        return analyzer_class(**kwargs)
    
    @classmethod
    def list_analyzers(cls):
        """列出所有可用的分析器"""
        return list(cls._analyzers.keys())

# 使用装饰器注册分析器
@AnalyzerRegistry.register("overall")
class OverallAnalysis(Analyzer):
    # 实现细节...

@AnalyzerRegistry.register("columnwise") 
class ColumnWiseAnalysis(Analyzer):
    # 实现细节...
```

### 6.3.2 分析执行器

分析执行器负责协调分析流程：

```python
class AnalysisExecutor:
    """分析执行器"""
    
    def __init__(self, dataset, analyzers_config):
        self.dataset = dataset
        self.analyzers_config = analyzers_config
        self.results = {}
    
    def execute(self):
        """执行分析流程"""
        
        for analyzer_name, config in self.analyzers_config.items():
            try:
                # 获取分析器实例
                analyzer = AnalyzerRegistry.get_analyzer(analyzer_name, **config)
                
                # 执行分析
                logger.info(f"Executing analyzer: {analyzer_name}")
                result = analyzer.analyze(self.dataset)
                
                # 存储结果
                self.results[analyzer_name] = result
                
                logger.info(f"Analyzer {analyzer_name} completed successfully")
                
            except Exception as e:
                logger.error(f"Analyzer {analyzer_name} failed: {e}")
                self.results[analyzer_name] = {"error": str(e)}
        
        return self.results
    
    def generate_report(self, output_path=None):
        """生成分析报告"""
        report = AnalysisReport(self.results)
        
        if output_path:
            report.save(output_path)
        
        return report
```

### 6.3.3 分布式分析支持

对于大规模数据集，Data-Juicer支持分布式分析：

```python
class DistributedAnalysisExecutor:
    """分布式分析执行器"""
    
    def __init__(self, dataset, analyzers_config, num_workers=4):
        self.dataset = dataset
        self.analyzers_config = analyzers_config
        self.num_workers = num_workers
    
    def execute(self):
        """分布式执行分析"""
        
        # 分割数据集
        dataset_shards = self._split_dataset()
        
        # 使用Ray进行分布式计算
        if ray.is_initialized():
            return self._execute_with_ray(dataset_shards)
        else:
            # 回退到多进程
            return self._execute_with_multiprocessing(dataset_shards)
    
    def _split_dataset(self):
        """分割数据集"""
        shard_size = len(self.dataset) // self.num_workers
        shards = []
        
        for i in range(self.num_workers):
            start_idx = i * shard_size
            end_idx = start_idx + shard_size if i < self.num_workers - 1 else len(self.dataset)
            shards.append(self.dataset.select(range(start_idx, end_idx)))
        
        return shards
    
    def _execute_with_ray(self, dataset_shards):
        """使用Ray执行分布式分析"""
        
        @ray.remote
        def analyze_shard(shard, analyzers_config):
            """分析单个数据分片"""
            executor = AnalysisExecutor(shard, analyzers_config)
            return executor.execute()
        
        # 并行执行分析任务
        futures = [
            analyze_shard.remote(shard, self.analyzers_config)
            for shard in dataset_shards
        ]
        
        # 收集结果
        shard_results = ray.get(futures)
        
        # 合并结果
        return self._merge_results(shard_results)
    
    def _merge_results(self, shard_results):
        """合并分片分析结果"""
        merged_results = {}
        
        for analyzer_name in self.analyzers_config.keys():
            # 收集所有分片的该分析器结果
            analyzer_results = [
                result[analyzer_name] for result in shard_results
                if analyzer_name in result and "error" not in result[analyzer_name]
            ]
            
            if analyzer_results:
                # 合并统计信息
                merged_results[analyzer_name] = self._merge_analyzer_results(analyzer_results)
        
        return merged_results
```

## 6.4 可视化与报告生成

### 6.4.1 可视化引擎

Data-Juicer提供丰富的可视化功能：

```python
class VisualizationEngine:
    """可视化引擎"""
    
    def __init__(self):
        self.plotters = {
            "histogram": self._plot_histogram,
            "bar_chart": self._plot_bar_chart,
            "scatter_plot": self._plot_scatter_plot,
            "heatmap": self._plot_heatmap
        }
    
    def visualize(self, data, plot_type="histogram", **kwargs):
        """生成可视化图表"""
        
        if plot_type not in self.plotters:
            raise ValueError(f"Unsupported plot type: {plot_type}")
        
        plotter = self.plotters[plot_type]
        return plotter(data, **kwargs)
    
    def _plot_histogram(self, data, title="Histogram", xlabel="Value", ylabel="Frequency"):
        """绘制直方图"""
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(10, 6))
        plt.hist(data, bins=50, alpha=0.7, color='skyblue')
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.grid(True, alpha=0.3)
        
        return plt.gcf()
    
    def _plot_heatmap(self, correlation_matrix, title="Correlation Heatmap"):
        """绘制热力图"""
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(
            correlation_matrix, 
            annot=True, 
            cmap='coolwarm', 
            center=0,
            square=True
        )
        plt.title(title)
        
        return plt.gcf()
```

### 6.4.2 分析报告生成

```python
class AnalysisReport:
    """分析报告生成器"""
    
    def __init__(self, analysis_results):
        self.results = analysis_results
        self.visualizer = VisualizationEngine()
    
    def generate_summary(self):
        """生成摘要报告"""
        summary = {
            "analysis_timestamp": datetime.now().isoformat(),
            "total_analyzers": len(self.results),
            "analyzers_executed": list(self.results.keys()),
            "key_findings": self._extract_key_findings()
        }
        
        return summary
    
    def _extract_key_findings(self):
        """提取关键发现"""
        findings = []
        
        for analyzer_name, result in self.results.items():
            if "error" in result:
                findings.append(f"{analyzer_name}: Analysis failed - {result['error']}")
                continue
            
            if analyzer_name == "overall":
                dataset_size = result.get("dataset_size", 0)
                avg_length = result.get("avg_text_length", 0)
                findings.append(f"Dataset size: {dataset_size}, Average text length: {avg_length:.2f}")
            
            elif analyzer_name == "columnwise":
                for column, stats in result.items():
                    if stats.get("type") == "text":
                        unique_ratio = stats.get("unique_count", 0) / max(1, stats.get("count", 0))
                        findings.append(f"Column {column}: {stats.get('unique_count')} unique values ({unique_ratio:.1%})")
        
        return findings
    
    def save(self, output_path):
        """保存报告到文件"""
        
        # 创建报告目录
        os.makedirs(output_path, exist_ok=True)
        
        # 生成HTML报告
        html_report = self._generate_html_report()
        
        # 保存HTML文件
        with open(os.path.join(output_path, "analysis_report.html"), "w") as f:
            f.write(html_report)
        
        # 保存可视化图表
        self._save_visualizations(output_path)
        
        # 保存原始数据（JSON格式）
        with open(os.path.join(output_path, "analysis_results.json"), "w") as f:
            json.dump(self.results, f, indent=2, default=str)
    
    def _generate_html_report(self):
        """生成HTML格式的报告"""
        
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Data-Juicer Analysis Report</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .section { margin-bottom: 30px; }
                .finding { background: #f0f8ff; padding: 10px; margin: 5px 0; }
                table { border-collapse: collapse; width: 100%; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
            </style>
        </head>
        <body>
            <h1>Data-Juicer Analysis Report</h1>
            <div class="section">
                <h2>Summary</h2>
                <p><strong>Analysis Time:</strong> {timestamp}</p>
                <p><strong>Analyzers Executed:</strong> {analyzers}</p>
            </div>
            
            <div class="section">
                <h2>Key Findings</h2>
                {findings}
            </div>
            
            <div class="section">
                <h2>Detailed Results</h2>
                {detailed_results}
            </div>
        </body>
        </html>
        """
        
        summary = self.generate_summary()
        findings_html = "\n".join(
            f'<div class="finding">{finding}</div>' 
            for finding in summary["key_findings"]
        )
        
        return html_template.format(
            timestamp=summary["analysis_timestamp"],
            analyzers=", ".join(summary["analyzers_executed"]),
            findings=findings_html,
            detailed_results=self._generate_detailed_results_html()
        )
```

## 6.5 实战案例：文本质量分析

### 6.5.1 自定义文本质量分析器

```python
@AnalyzerRegistry.register("text_quality")
class TextQualityAnalyzer(Analyzer):
    """文本质量分析器"""
    
    def __init__(self, quality_metrics=None):
        self.quality_metrics = quality_metrics or [
            "readability", "grammar", "coherence", "relevance"
        ]
        self.stats = {}
    
    def analyze(self, dataset, **kwargs):
        """分析文本质量"""
        
        quality_scores = {metric: [] for metric in self.quality_metrics}
        
        for sample in dataset:
            text = sample.get("text", "")
            
            # 计算各项质量指标
            for metric in self.quality_metrics:
                score = self._compute_quality_score(text, metric)
                quality_scores[metric].append(score)
        
        # 汇总统计信息
        self.stats = {
            "quality_distribution": self._compute_distribution(quality_scores),
            "average_scores": self._compute_averages(quality_scores),
            "correlation_analysis": self._analyze_correlations(quality_scores)
        }
        
        return self.stats
    
    def _compute_quality_score(self, text, metric):
        """计算特定质量指标得分"""
        
        if metric == "readability":
            return self._compute_readability_score(text)
        elif metric == "grammar":
            return self._compute_grammar_score(text)
        elif metric == "coherence":
            return self._compute_coherence_score(text)
        elif metric == "relevance":
            return self._compute_relevance_score(text)
        else:
            return 0.5  # 默认得分
    
    def _compute_readability_score(self, text):
        """计算可读性得分"""
        # 实现Flesch Reading Ease等可读性指标
        words = text.split()
        sentences = text.split('.')
        
        if len(words) == 0 or len(sentences) == 0:
            return 0
        
        # 简化的可读性计算
        avg_sentence_length = len(words) / len(sentences)
        readability = max(0, min(1, 1 - (avg_sentence_length - 10) / 50))
        
        return readability
```

### 6.5.2 集成分析流程

```python
def analyze_text_quality_pipeline(dataset_path, output_dir):
    """文本质量分析管道"""
    
    # 加载数据集
    dataset = load_dataset(dataset_path)
    
    # 配置分析器
    analyzers_config = {
        "overall": {},
        "columnwise": {"columns": ["text", "quality_score"]},
        "text_quality": {
            "quality_metrics": ["readability", "grammar", "coherence"]
        }
    }
    
    # 执行分析
    executor = AnalysisExecutor(dataset, analyzers_config)
    results = executor.execute()
    
    # 生成报告
    report = executor.generate_report(output_dir)
    
    # 生成可视化图表
    visualizer = VisualizationEngine()
    
    # 质量得分分布图
    quality_scores = results["text_quality"]["quality_distribution"]
    for metric, scores in quality_scores.items():
        fig = visualizer.visualize(scores, "histogram", 
                                 title=f"{metric} Score Distribution")
        fig.savefig(os.path.join(output_dir, f"{metric}_distribution.png"))
    
    return results, report
```

## 6.6 性能优化与最佳实践

### 6.6.1 分析性能优化

```python
class OptimizedAnalysisExecutor(AnalysisExecutor):
    """优化版分析执行器"""
    
    def __init__(self, dataset, analyzers_config, optimization_config=None):
        super().__init__(dataset, analyzers_config)
        self.optimization_config = optimization_config or {}
    
    def execute(self):
        """优化执行分析"""
        
        # 预计算共享特征
        shared_features = self._precompute_shared_features()
        
        # 并行执行独立分析器
        independent_results = self._execute_independent_analyzers(shared_features)
        
        # 顺序执行依赖分析器
        dependent_results = self._execute_dependent_analyzers(shared_features, independent_results)
        
        # 合并结果
        self.results = {**independent_results, **dependent_results}
        
        return self.results
    
    def _precompute_shared_features(self):
        """预计算共享特征"""
        features = {}
        
        # 识别分析器间的共享计算
        common_operations = self._identify_common_operations()
        
        for operation in common_operations:
            if operation == "text_length":
                features["text_lengths"] = [
                    len(sample.get("text", "")) for sample in self.dataset
                ]
            elif operation == "token_count":
                features["token_counts"] = self._compute_token_counts()
        
        return features
    
    def _identify_common_operations(self):
        """识别共享计算操作"""
        common_ops = set()
        
        for analyzer_config in self.analyzers_config.values():
            if "requires_text_length" in analyzer_config:
                common_ops.add("text_length")
            if "requires_token_count" in analyzer_config:
                common_ops.add("token_count")
        
        return list(common_ops)
```

### 6.6.2 内存优化策略

```python
def memory_efficient_analysis(dataset, analyzers_config, batch_size=1000):
    """内存高效的分析执行"""
    
    results = {}
    
    # 分批处理数据
    for i in range(0, len(dataset), batch_size):
        batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
        
        # 执行分析
        batch_executor = AnalysisExecutor(batch, analyzers_config)
        batch_results = batch_executor.execute()
        
        # 增量合并结果
        results = self._merge_incremental_results(results, batch_results)
        
        # 清理内存
        del batch
        import gc
        gc.collect()
    
    return results

def _merge_incremental_results(self, current_results, new_results):
    """增量合并分析结果"""
    merged = {}
    
    for analyzer_name, new_result in new_results.items():
        if analyzer_name not in current_results:
            merged[analyzer_name] = new_result
        else:
            # 合并统计信息（如平均值、分布等）
            merged[analyzer_name] = self._merge_analyzer_results(
                [current_results[analyzer_name], new_result]
            )
    
    return merged
```

Data-Juicer的分析功能提供了强大的数据洞察能力，通过灵活的架构设计和丰富的分析器类型，帮助用户深入理解数据集特征，为数据质量评估和预处理策略制定提供科学依据。