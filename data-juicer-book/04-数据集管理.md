# 第4章：数据集管理

[← 上一章](03-算子系统详解.md) | [返回目录](00-目录与索引.md) | [下一章 →](05-模型调用机制.md)

---

## 4.1 DJDataset设计架构

Data-Juicer的数据集管理系统采用了层次化设计，提供了灵活、高效的数据访问和处理能力。核心设计理念是"统一接口、分层实现、性能优化"。

### 层次化架构设计

```python
# 数据集层次结构
class DJDataset(ABC):
    """数据集抽象基类"""
    
class NestedDataset(DJDataset):
    """嵌套数据集实现"""
    
class RayDataset(DJDataset):
    """Ray分布式数据集"""
```

### 核心接口设计

DJDataset基类定义了统一的数据集接口：

```python
class DJDataset(ABC):
    """数据集抽象基类"""
    
    @abstractmethod
    def __len__(self):
        """获取数据集大小"""
        pass
    
    @abstractmethod
    def __getitem__(self, index):
        """获取单个样本"""
        pass
    
    @abstractmethod
    def column_names(self):
        """获取列名列表"""
        pass
    
    @abstractmethod
    def select(self, indices):
        """选择子集"""
        pass
    
    @abstractmethod
    def map(self, function, **kwargs):
        """映射操作"""
        pass
    
    @abstractmethod
    def filter(self, function, **kwargs):
        """过滤操作"""
        pass
```

## 4.2 NestedDataset实现

NestedDataset是Data-Juicer的核心数据集实现，支持复杂的数据结构和高效的嵌套访问。

### 嵌套访问机制

```python
class NestedDataset(DJDataset):
    """嵌套数据集实现"""
    
    def __init__(self, data, column_names=None):
        self.data = data
        self.column_names = column_names or self._infer_column_names()
        self._cached_indices = None
    
    def __getitem__(self, index):
        """支持多种索引方式"""
        
        # 整数索引
        if isinstance(index, int):
            return self._get_single_sample(index)
        
        # 切片索引
        elif isinstance(index, slice):
            return self._get_slice_samples(index)
        
        # 列表索引
        elif isinstance(index, (list, np.ndarray)):
            return self._get_multiple_samples(index)
        
        # 布尔索引
        elif isinstance(index, (bool, np.bool_)):
            if index:
                return self  # 返回整个数据集
            else:
                return NestedDataset([])  # 返回空数据集
        
        else:
            raise TypeError(f"Unsupported index type: {type(index)}")
    
    def _get_single_sample(self, index):
        """获取单个样本"""
        if index < 0 or index >= len(self):
            raise IndexError(f"Index {index} out of range")
        
        # 支持多种数据格式
        if isinstance(self.data, list):
            return self.data[index]
        elif isinstance(self.data, dict):
            return {key: values[index] for key, values in self.data.items()}
        else:
            # 自定义数据结构的处理
            return self._custom_get_item(index)
    
    def _custom_get_item(self, index):
        """自定义数据结构的索引实现"""
        # 这里可以扩展支持更多数据格式
        if hasattr(self.data, '__getitem__'):
            return self.data[index]
        else:
            raise NotImplementedError("Unsupported data format")
```

### 嵌套字段访问

NestedDataset支持复杂的嵌套字段访问：

```python
def get_nested_field(self, field_path):
    """获取嵌套字段"""
    
    def _extract_field(sample, path_parts):
        """递归提取嵌套字段"""
        if not path_parts:
            return sample
        
        current_field = path_parts[0]
        remaining_path = path_parts[1:]
        
        # 支持字典访问
        if isinstance(sample, dict) and current_field in sample:
            return _extract_field(sample[current_field], remaining_path)
        
        # 支持对象属性访问
        elif hasattr(sample, current_field):
            return _extract_field(getattr(sample, current_field), remaining_path)
        
        # 支持列表索引访问
        elif isinstance(sample, (list, tuple)) and current_field.isdigit():
            idx = int(current_field)
            if 0 <= idx < len(sample):
                return _extract_field(sample[idx], remaining_path)
        
        # 默认返回None
        return None
    
    # 分割字段路径
    path_parts = field_path.split('.')
    
    # 为所有样本提取该字段
    results = []
    for i in range(len(self)):
        sample = self[i]
        value = _extract_field(sample, path_parts)
        results.append(value)
    
    return results
```

## 4.3 数据处理流程

### 数据加载与初始化

```python
def load_dataset(data_source, **kwargs):
    """加载数据集"""
    
    # 支持多种数据源
    if isinstance(data_source, str):
        # 文件路径
        if data_source.endswith('.jsonl'):
            return load_jsonl_dataset(data_source, **kwargs)
        elif data_source.endswith('.csv'):
            return load_csv_dataset(data_source, **kwargs)
        elif data_source.endswith('.parquet'):
            return load_parquet_dataset(data_source, **kwargs)
        else:
            # 尝试自动检测格式
            return auto_detect_format(data_source, **kwargs)
    
    elif isinstance(data_source, (list, dict)):
        # 内存数据
        return NestedDataset(data_source, **kwargs)
    
    elif hasattr(data_source, '__iter__'):
        # 可迭代对象
        return NestedDataset(list(data_source), **kwargs)
    
    else:
        raise ValueError(f"Unsupported data source: {type(data_source)}")

def load_jsonl_dataset(file_path, **kwargs):
    """加载JSONL格式数据集"""
    data = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                sample = json.loads(line.strip())
                data.append(sample)
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse JSON line: {e}")
    
    return NestedDataset(data, **kwargs)
```

### 数据转换与映射

```python
def map_dataset(dataset, function, batched=False, batch_size=1000, **kwargs):
    """映射数据集"""
    
    if batched:
        return _map_batched(dataset, function, batch_size, **kwargs)
    else:
        return _map_single(dataset, function, **kwargs)

def _map_batched(dataset, function, batch_size, **kwargs):
    """批量映射"""
    results = []
    
    for i in range(0, len(dataset), batch_size):
        batch_indices = range(i, min(i + batch_size, len(dataset)))
        batch = dataset.select(batch_indices)
        
        # 应用映射函数
        processed_batch = function(batch, **kwargs)
        results.extend(processed_batch)
    
    return NestedDataset(results)

def _map_single(dataset, function, **kwargs):
    """单样本映射"""
    results = []
    
    for i in range(len(dataset)):
        sample = dataset[i]
        processed_sample = function(sample, **kwargs)
        results.append(processed_sample)
    
    return NestedDataset(results)
```

## 4.4 DJ格式支持

Data-Juicer定义了专用的DJ格式，支持高效的数据存储和快速访问。

### DJ格式规范

```python
class DJFormat:
    """DJ格式规范"""
    
    # 文件结构
    STRUCTURE = {
        "metadata.json": "元数据文件",
        "data.parquet": "数据文件（Parquet格式）",
        "indices.json": "索引文件",
        "statistics.json": "统计信息文件"
    }
    
    # 元数据格式
    METADATA_SCHEMA = {
        "version": "格式版本",
        "created_at": "创建时间",
        "dataset_info": {
            "name": "数据集名称",
            "description": "数据集描述",
            "size": "数据集大小",
            "columns": "列信息"
        },
        "processing_history": "处理历史",
        "statistics": "统计信息摘要"
    }
```

### DJ格式读写

```python
def save_as_dj_format(dataset, output_dir, **kwargs):
    """保存为DJ格式"""
    
    # 创建输出目录
    os.makedirs(output_dir, exist_ok=True)
    
    # 保存元数据
    metadata = _generate_metadata(dataset, **kwargs)
    with open(os.path.join(output_dir, "metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # 保存数据（Parquet格式）
    data_file = os.path.join(output_dir, "data.parquet")
    _save_as_parquet(dataset, data_file)
    
    # 保存索引
    indices = _generate_indices(dataset)
    with open(os.path.join(output_dir, "indices.json"), 'w') as f:
        json.dump(indices, f, indent=2)
    
    # 保存统计信息
    statistics = _compute_statistics(dataset)
    with open(os.path.join(output_dir, "statistics.json"), 'w') as f:
        json.dump(statistics, f, indent=2)

def load_dj_format(input_dir, **kwargs):
    """加载DJ格式数据集"""
    
    # 验证文件结构
    _validate_dj_structure(input_dir)
    
    # 加载元数据
    metadata_path = os.path.join(input_dir, "metadata.json")
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    # 加载数据
    data_file = os.path.join(input_dir, "data.parquet")
    data = _load_parquet_data(data_file)
    
    # 创建数据集实例
    dataset = NestedDataset(data, **kwargs)
    
    # 设置元数据
    dataset.metadata = metadata
    
    return dataset
```

## 4.5 缓存与性能优化

### 智能缓存系统

```python
class DatasetCache:
    """数据集缓存管理器"""
    
    def __init__(self, max_size=1000, cache_dir=None):
        self.max_size = max_size
        self.cache_dir = cache_dir or tempfile.mkdtemp()
        self.cache = {}
        self.access_times = {}
        self.hits = 0
        self.misses = 0
    
    def get(self, key, loader_func=None):
        """获取缓存数据"""
        
        # 检查内存缓存
        if key in self.cache:
            self.hits += 1
            self.access_times[key] = time.time()
            return self.cache[key]
        
        # 检查磁盘缓存
        disk_path = self._get_disk_path(key)
        if os.path.exists(disk_path):
            try:
                data = self._load_from_disk(disk_path)
                self.cache[key] = data
                self.hits += 1
                self.access_times[key] = time.time()
                return data
            except Exception as e:
                logger.warning(f"Failed to load from disk cache: {e}")
        
        # 缓存未命中，加载数据
        self.misses += 1
        if loader_func:
            data = loader_func()
            self.put(key, data)
            return data
        else:
            return None
    
    def put(self, key, data):
        """添加数据到缓存"""
        
        # 检查缓存大小，必要时清理
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        # 添加到内存缓存
        self.cache[key] = data
        self.access_times[key] = time.time()
        
        # 保存到磁盘缓存
        disk_path = self._get_disk_path(key)
        try:
            self._save_to_disk(data, disk_path)
        except Exception as e:
            logger.warning(f"Failed to save to disk cache: {e}")
    
    def _evict_oldest(self):
        """清理最久未使用的缓存项"""
        if not self.access_times:
            return
        
        oldest_key = min(self.access_times, key=self.access_times.get)
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
        
        # 清理磁盘缓存
        disk_path = self._get_disk_path(oldest_key)
        if os.path.exists(disk_path):
            try:
                os.remove(disk_path)
            except Exception as e:
                logger.warning(f"Failed to remove disk cache: {e}")
```

### 性能优化策略

```python
class PerformanceOptimizer:
    """性能优化器"""
    
    def __init__(self, dataset, optimization_config=None):
        self.dataset = dataset
        self.config = optimization_config or {}
        self.cache = DatasetCache()
    
    def optimize_access_pattern(self):
        """优化数据访问模式"""
        
        # 预加载常用数据
        if self.config.get("preload_frequently_accessed", False):
            self._preload_frequent_data()
        
        # 数据分块
        if self.config.get("enable_chunking", True):
            self.dataset = self._chunk_dataset()
        
        # 索引优化
        if self.config.get("build_indices", True):
            self._build_indices()
    
    def _preload_frequent_data(self):
        """预加载常用数据"""
        # 分析访问模式，预加载热点数据
        frequent_columns = self._analyze_access_pattern()
        
        for column in frequent_columns:
            data = self.dataset[column]
            self.cache.put(f"column_{column}", data)
    
    def _chunk_dataset(self, chunk_size=10000):
        """数据分块"""
        # 将大数据集分割为小块，提高内存效率
        chunks = []
        
        for i in range(0, len(self.dataset), chunk_size):
            chunk = self.dataset.select(range(i, min(i + chunk_size, len(self.dataset))))
            chunks.append(chunk)
        
        return ChunkedDataset(chunks)
    
    def _build_indices(self):
        """构建索引"""
        # 为常用查询字段构建索引
        indexable_columns = self._identify_indexable_columns()
        
        for column in indexable_columns:
            index = {}
            for i, value in enumerate(self.dataset[column]):
                if value not in index:
                    index[value] = []
                index[value].append(i)
            
            self.cache.put(f"index_{column}", index)
```

## 4.6 检查点与恢复机制

### 检查点系统

```python
class CheckpointManager:
    """检查点管理器"""
    
    def __init__(self, checkpoint_dir, save_interval=1000):
        self.checkpoint_dir = checkpoint_dir
        self.save_interval = save_interval
        self.last_checkpoint = None
        os.makedirs(checkpoint_dir, exist_ok=True)
    
    def save_checkpoint(self, dataset, step, metadata=None):
        """保存检查点"""
        
        checkpoint_id = f"checkpoint_{step:08d}"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_id)
        
        # 保存数据集状态
        dataset.save(checkpoint_path)
        
        # 保存元数据
        checkpoint_metadata = {
            "step": step,
            "timestamp": time.time(),
            "dataset_size": len(dataset),
            "custom_metadata": metadata or {}
        }
        
        metadata_path = os.path.join(checkpoint_path, "metadata.json")
        with open(metadata_path, 'w') as f:
            json.dump(checkpoint_metadata, f, indent=2)
        
        self.last_checkpoint = checkpoint_id
        
        # 清理旧检查点
        self._cleanup_old_checkpoints()
    
    def load_checkpoint(self, checkpoint_id=None):
        """加载检查点"""
        
        if checkpoint_id is None:
            checkpoint_id = self._find_latest_checkpoint()
        
        if checkpoint_id is None:
            return None
        
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_id)
        
        # 加载数据集
        dataset = DJDataset.load(checkpoint_path)
        
        # 加载元数据
        metadata_path = os.path.join(checkpoint_path, "metadata.json")
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        return dataset, metadata
    
    def _find_latest_checkpoint(self):
        """查找最新的检查点"""
        if not os.path.exists(self.checkpoint_dir):
            return None
        
        checkpoints = []
        for item in os.listdir(self.checkpoint_dir):
            if item.startswith("checkpoint_"):
                checkpoints.append(item)
        
        if not checkpoints:
            return None
        
        # 按步骤排序，返回最新的
        checkpoints.sort(key=lambda x: int(x.split('_')[1]))
        return checkpoints[-1]
```

### 恢复机制

```python
def resume_from_checkpoint(checkpoint_dir, resume_step=None):
    """从检查点恢复处理"""
    
    checkpoint_manager = CheckpointManager(checkpoint_dir)
    
    # 查找检查点
    dataset, metadata = checkpoint_manager.load_checkpoint(resume_step)
    
    if dataset is None:
        logger.info("No checkpoint found, starting from scratch")
        return None, None
    
    logger.info(f"Resuming from step {metadata['step']}")
    
    # 恢复处理状态
    processing_state = {
        "current_step": metadata["step"],
        "dataset_size": metadata["dataset_size"],
        "custom_state": metadata.get("custom_metadata", {})
    }
    
    return dataset, processing_state
```

## 4.7 关键设计模式

### 适配器模式

```python
class DatasetAdapter:
    """数据集适配器，统一不同数据源的接口"""
    
    def __init__(self, data_source):
        self.data_source = data_source
        self._adapted_dataset = None
    
    def adapt(self):
        """适配数据源"""
        
        if isinstance(self.data_source, DJDataset):
            # 已经是DJDataset，直接返回
            self._adapted_dataset = self.data_source
        
        elif hasattr(self.data_source, 'to_pandas'):
            # Pandas DataFrame
            self._adapted_dataset = self._adapt_pandas(self.data_source)
        
        elif hasattr(self.data_source, 'to_dict'):
            # 字典格式
            self._adapted_dataset = self._adapt_dict(self.data_source)
        
        else:
            # 尝试通用适配
            self._adapted_dataset = self._adapt_generic(self.data_source)
        
        return self._adapted_dataset
    
    def _adapt_pandas(self, df):
        """适配Pandas DataFrame"""
        data = df.to_dict('records')
        return NestedDataset(data)
    
    def _adapt_dict(self, data_dict):
        """适配字典数据"""
        # 将列式字典转换为行式列表
        if all(isinstance(v, (list, np.ndarray)) for v in data_dict.values()):
            # 列式数据
            n_samples = len(next(iter(data_dict.values())))
            data = []
            for i in range(n_samples):
                sample = {k: v[i] for k, v in data_dict.items()}
                data.append(sample)
            return NestedDataset(data)
        else:
            # 行式数据
            return NestedDataset([data_dict])
```

### 装饰器模式

```python
class DatasetDecorator(DJDataset):
    """数据集装饰器基类"""
    
    def __init__(self, decorated_dataset):
        self._decorated = decorated_dataset
    
    def __len__(self):
        return len(self._decorated)
    
    def __getitem__(self, index):
        return self._decorated[index]
    
    def column_names(self):
        return self._decorated.column_names()

class CachedDataset(DatasetDecorator):
    """缓存装饰器"""
    
    def __init__(self, decorated_dataset, cache_size=1000):
        super().__init__(decorated_dataset)
        self.cache = {}
        self.cache_size = cache_size
        self.access_order = []
    
    def __getitem__(self, index):
        if index in self.cache:
            # 缓存命中
            self._update_access_order(index)
            return self.cache[index]
        
        # 缓存未命中
        data = self._decorated[index]
        self._add_to_cache(index, data)
        return data
    
    def _update_access_order(self, index):
        """更新访问顺序"""
        if index in self.access_order:
            self.access_order.remove(index)
        self.access_order.append(index)
    
    def _add_to_cache(self, index, data):
        """添加到缓存"""
        if len(self.cache) >= self.cache_size:
            # 清理最久未使用的缓存项
            oldest_index = self.access_order.pop(0)
            del self.cache[oldest_index]
        
        self.cache[index] = data
        self.access_order.append(index)
```

Data-Juicer的数据集管理系统通过精心设计的架构和丰富的功能，为大规模数据处理提供了强大的支持。其模块化设计和性能优化策略使得系统能够高效处理各种规模的数据集。